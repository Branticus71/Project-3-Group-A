---
title: "Project-3-Group-A"
author: "Brant Armstrong, Naman Goel"
date: "2022-11-06"
output: html_document
---
# Necessary Packages
```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(caret)
library(corrplot)
library(corrr)
library(psych)
library(rmarkdown)
library(kableExtra)
library(timereg)
```
# Introduction

You should have an introduction section that briefly describes the data and the variables you have to work
with (just discuss the ones you want to use). Your target variables is the shares variable.
You should also mention the purpose of your analysis and the methods you’ll use to model the response.
You’ll describe those in more detail later.
This section should be done by the ‘second’ group member.

# Reading in Data
Need to decide on what variables we will use to predict log_shares. Some ideas below:

1. num_imgs: Number of images
2. n_tokens_content: Number of words in the content
3. n_unique_tokens: Rate of unique words in the content
4. rate_positive_words: Rate of positive words among non-neutral tokens
5. rate_negative_words: Rate of negative words among non-neutral tokens
6. Discrete variables for each day
7. num_videos: Number of videos
8. num_hrefs: Number of links
9. kw_avg_avg: Avg. keyword (avg. shares)
10. n_tokens_title: Number of words in the title
11. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable

```{r}
df <- read_csv("./OnlineNewsPopularity.csv")
#Creating a new variable for to contain data channels and removing the old one.
df <- df %>%
  mutate(data_channel = if_else(
    data_channel_is_bus == 1,
    "Business",
    if_else(
      data_channel_is_entertainment == 1,
      "Entertainment",
      if_else(
        data_channel_is_lifestyle == 1,
        "Lifestyle",
        if_else(
          data_channel_is_socmed == 1,
          "Socmed",
          if_else(data_channel_is_tech == 1, "Tech", "World")))))) %>% 
  #Removing non-predictors and old data channel variables
  select(-c(url, timedelta, data_channel_is_bus, data_channel_is_entertainment,
            data_channel_is_socmed, data_channel_is_tech, data_channel_is_world,
            data_channel_is_lifestyle)) %>%
  #Changing shares to log(shares) for better graphing and models
  mutate(log_shares = log(shares)) %>%
  select(-shares) %>%
  #Renaming the day variables
  rename(monday = weekday_is_monday , tuesday = weekday_is_tuesday, wednesday = weekday_is_wednesday,          thursday = weekday_is_thursday, friday = weekday_is_friday, saturday = weekday_is_saturday,           sunday = weekday_is_sunday)
  
#Subsetting to just a single data_channel type
df_filtered <- df %>%
                filter(data_channel == "Lifestyle") 
#Creating the training and test datasets
index_train <- createDataPartition(df_filtered$log_shares, p = .7, list = FALSE )
df_train <- df_filtered[index_train,]
df_test <- df_filtered[-index_train,]

#Creating a data frame containing a categorical day variable for graphs
categ_day <- df_train %>%
  mutate(day = if_else(
    monday == 1,
    "Monday",
    if_else(
      tuesday == 1,
      "Tuesday",
      if_else(
        wednesday == 1,
        "Wednesday",
        if_else(
          thursday == 1,
          "Thursday",
          if_else(
            friday == 1,
            "Friday",
            if_else(saturday == 1,
                    "Saturday", "Sunday")))))))

#Removing variables that aren't continuous for tables and principal components
df_continuous <- df_train %>%
  select(-c(monday, tuesday, wednesday, thursday,friday, saturday, sunday, is_weekend, data_channel,
            n_non_stop_words))
```

# Summarization

## Boxplot of Log(shares) by Day.
```{r}
#Box plot of log_shares by day
ggplot(categ_day, aes(x = day, y = log_shares, col = day)) + 
  geom_boxplot(fill="grey") + 
  geom_jitter() + 
  ylab("Log Shares") + 
  xlab("") +
  theme(axis.text.x = element_text(angle = 45)) +
  ggtitle("Boxplot for Log Shares by Day")
```

We can see if there is a perceived effect of the `day` the article is published on shares by looking at the behavior of each boxplot. If certain days have higher or lower medians along with different size whiskers then there may be a relationship between publishing `day` and the amount of shares an article gets.


## Scatterplot of all Continuous Predictors Versus Log(shares) in a Facet Wrap. 
```{r, fig.height=9}
interest_vars <- c("num_imgs", "n_tokens_content", "n_unique_tokens", "rate_positive_words",
                   "rate_negative_words", "num_videos", "num_hrefs", "kw_avg_avg", "n_tokens_title", "self_reference_avg_sharess")
categ_day %>%
  gather(interest_vars, key = "variables", value = "value") %>%
  ggplot(aes(x = value, y = log_shares, color = day)) +
    geom_jitter() +
    stat_smooth(method = "lm") +
    facet_wrap(~ variables, scales = "free", shrink = FALSE, ncol=2  )
```

For these graphs, it helps to focus on each line and it's color. If all lines are generally negative in slope then we expect there to be a negative relationship between that variable and the number of shares while positive slope means a positive relationship. If the lines differ quite a bit between color then there is most likely an interaction between that variable, the `day` of release, and shares. Also, keep in mind that any outliers could have a strong effect on the creation of the line and in turn it's slope potentially misrepresenting the relationship between that variable and shares. 



## Correlation Plot of Each Variable with Shares
```{r}
#Need to limit to just predictors of interest once those are decided on
cor_train<- df_train %>%
  correlate() %>%
  focus(log_shares)
paged_table(cor_train)

cor_train %>% 
  mutate(term = factor(term, levels = term[order(log_shares)])) %>% 
  ggplot(aes(x = term, y = log_shares)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with log(Shares)") +
    xlab("Variable") +
  theme(axis.text.x = element_text(angle = 90)) 
```

## Sentiment Plot

These plots attempt to find the trend of log of shares as a function of average positive and average negative polarity. If we see that the number of shares increases or decrease with the increase in the polarity of the content, then we can infer the trend from the slope of the linear regression line on the plots given below.

```{r}
df_train$Popularity <-qcut(df_train$log_shares,
                           cuts=5,
                           label=c('Very Low','Low','Average','High','Very High'))
ggplot(df_train, aes(avg_positive_polarity,log_shares))+ 
  geom_point(aes(color=Popularity)) + 
  geom_smooth(method="lm",color='black')+
  labs(x="Average positive polarity",y="Log of shares")
```

## Average number of shares per words in title

A plot of average number of log shares vs number of words in the title is generated. Title is an important part and it is key to have adequate number and key words in the title. The plot helps us conclude whether the number of words in the title drives the log of shares.

```{r}
data_plot_3 <- df_train %>% 
  select(n_tokens_title, log_shares) %>% 
  group_by(n_tokens_title) %>% 
  summarise(mean_token_title = mean(log_shares))

ggplot(data_plot_3, aes(n_tokens_title, mean_token_title)) + 
  geom_line() +
  labs(x="Number of words in the title",y="Average number of shares")
```
## Histogram

```{r}
plot2 <- df_train %>% 
  select(day, log_shares) %>% 
  group_by(day) %>% 
  summarise(Avg_Shares = mean(log_shares))

ggplot(data = plot2, aes(day, Avg_Shares)) + 
  labs(x="Days of Week",y="Number OF Shares",title="Average log of shares per day") + 
  geom_col(fill="violet",width=0.4)+
  theme(plot.title = element_text(hjust = 0.5))
```

Fairly self explanatory graph, the variables most correlated with shares will most likely be some of the strongest predictors in the models; particularly the linear regressions.

## Summary statistics using the `describe` function from the `psych` package sorted by standard deviation.
```{r}
tab <-  describe(df_continuous, fast = TRUE)
#Arranging by Standard Deviation and rounding to two places
table_continuous <- tab %>%
  arrange(desc(sd)) %>%
  modify_if( ~is.numeric(.), ~round(.,2))
#Paged table to allow reader to peruse all variables
paged_table(table_continuous)
```
## Contingency table using created categorical variables

```{r}
# Creating new Categorical Variables

df_train$videos_range <- ifelse(df_train$num_videos %in% c(0:30), "0-30",
                          ifelse(df_train$num_videos %in% c(31:60), "31-60",
                          ifelse(df_train$num_videos %in% c(61:90), "61-90","Null")))
df_train$videos_range <- ordered(as.factor(df_train$videos_range),
                        levels = c("0-30","31-60","61-90"))
df_train$day <- ifelse(df_train$monday == 1, "Monday",
                ifelse(df_train$tuesday == 1, "Tuesday",
                ifelse(df_train$wednesday == 1, "Wednesday", 
                ifelse(df_train$thursday == 1, "Thursday",
                ifelse(df_train$friday == 1, "Friday",
                ifelse(df_train$saturday == 1, "Saturday",
                ifelse(df_train$sunday == 1, "Sunday","Null")))))))
df_train$day <- ordered(as.factor(df_train$day),
                        levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
contingency_table <- table(df_train$day,df_train$videos_range)
contingency_table%>%
  kbl(caption="Table for Days and Number of Images") %>%
  kable_classic(full_width = F)
```

## Principal Components Screeplot
```{r}
#Excluding shares for creating principal components
df_no_shares <- df_continuous %>%
  select(-log_shares)
#Creating PC's along with center and scaling variables
PCs <- prcomp(df_no_shares, center = TRUE, scale = TRUE)
#Creating screeplots
par(mfrow = c(1,2))
plot(PCs$sdev^2/sum(PCs$sdev^2), xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = 'b')
plot(cumsum(PCs$sdev^2/sum(PCs$sdev^2)), xlab = "Principal Component",
ylab = "Cum. Prop of Variance Explained", ylim = c(0, 1), type = 'b')
#Selecting only the PC's up to a 80% variance explained threshold using caret
PCs_eighty <- preProcess(df_no_shares, method = c("center","scale", "pca"), thresh = .8)
#Creating a data frame with just my PC's, day variables, and log_shares to use later as a regression
df_PC <- predict(PCs_eighty, newdata = df_no_shares)
#Monday is excluded to avoid multicollinearity
df_PC <- df_PC %>%
  bind_cols(log_shares = df_continuous$log_shares,tuesday = df_train$tuesday, 
            wednesday = df_train$wednesday, thursday = df_train$thursday, friday = df_train$friday,
            saturday = df_train$saturday, sunday = df_train$sunday)

```

# Modeling

## Principal Components Regression
```{r}
set.seed(111)
fit_PC <- train(log_shares ~ ., data = df_PC, 
                 method = "lm",
                 trControl = trainControl(method = "cv" , number = 10))
summary(fit_PC)
#Creating a test data frame from the PCs then using that to test the PC regression. RMSE reported after.
test_PC <- predict(PCs_eighty, newdata = df_test)
pred_PC <- predict(fit_PC, newdata = test_PC)
postResample(pred_PC, obs = df_test$log_shares)
```

## Random Forest
```{r, warning = FALSE}
#May need to add or remove variables later
forest_vars <- c("log_shares","num_imgs", "n_tokens_content", "n_unique_tokens", "rate_positive_words",
                   "rate_negative_words", "num_videos", "num_hrefs", "kw_avg_avg", "n_tokens_title",
                   "tuesday", "wednesday","thursday","friday", "saturday", "sunday", "monday")
df_forest <- df_train %>%
  select(forest_vars)
set.seed(111)
fit_forest <- train(log_shares ~ ., data = df_forest, method = "treebag",
                 trControl = trainControl(method = "cv" , number = 10),
                 preProcess = c("center", "scale"),
                 mtry = c(1:16))
pred_forest <- predict(fit_forest, newdata = df_test)
postResample(pred_forest, df_test$log_shares)
```

```{r}
set.seed(111)
boost_fit <- train(shares ~ .,data = train_df2,
                   method = 'gbm',
                   preProcess = c("center", "scale"),
                   trControl = tr_ctrl,
                   verbose = FALSE)
```